
# Paper Notes on Pretrain Language Models with Factual Knowledge

<!-- omit in toc -->
## Contents

- [Paper Notes on Pretrain Language Models with Factual Knowledge](#paper-notes-on-pretrain-language-models-with-factual-knowledge)
  - [Introduction](#introduction)
    - [Keywords Convention](#keywords-convention)
    - [How to contribute?](#how-to-contribute)
  - [Papers](#papers)
    - [Model Warmup for ICL](#model-warmup-for-icl)
    - [Prompt Tuning for ICL](#prompt-tuning-for-icl)
      - [Prompt Selection Strategies for LLMs](#prompt-selection-strategies-for-llms)
    - [Prompt Formulation Strategies for LLMs](#prompt-formulation-strategies-for-llms)
    - [Analysis of ICL](#analysis-of-icl)
      - [Influence Factors for ICL](#influence-factors-for-icl)
      - [Working Mechanism of ICL](#working-mechanism-of-icl)
    - [Evaluation and Resources](#evaluation-and-resources)
    - [Application](#application)
    - [Challenges and Future Directions](#challenges-and-future-directions)
    - [Continual Pretraining](#continual-pretraining)
    - [Model Editing v.s. Continual Learning](#model-editing-vs-continual-learning)

## Introduction

This is a paper list about **In-context learning**.

### Keywords Convention

![](https://img.shields.io/badge/Meta-ICL) abbreviation

![](https://img.shields.io/badge/Efficient-EAD8D9) key features

![](https://img.shields.io/badge/QA-D8D0E1) main task

![](https://img.shields.io/badge/20220331-FAEFCA) editing info

### How to contribute?

Add new papers to the corresponding

## Papers

### Model Warmup for ICL

 1. A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models [[pdf](https://arxiv.org/pdf/2202.08772.pdf)]
 
 summarize the current progress of pre-trained language model based knowledge-enhanced models (PLMKEs) by dissecting their three vital elements: knowledge sources, knowledge-intensive NLP tasks, and
 knowledge fusion methods.
 main encyclopedic knowledge-intensive NLP tasks:
 - Open-domain QA
   - Natural Questions
   - HotpotQA
 - Fact Verification
   - FEVER
   - BOOLQ
 - Entity Linking
   - ACE2004
   - AIDA CoNLL-YAGO
   - WNWI
   - WNCW

### Prompt Tuning for ICL
#### Prompt Selection Strategies for LLMs
### Prompt Formulation Strategies for LLMs

### Analysis of ICL
#### Influence Factors for ICL
#### Working Mechanism of ICL



### Evaluation and Resources



### Application


### Challenges and Future Directions

This section contains the pilot works that might contributes to the prevalence of model editiing paradigm.

1. **Editable Neural Networks** ICLR 2020. ![](https://img.shields.io/badge/Editable_training-DCE7F1)

   *Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko*.  [[pdf](https://openreview.net/pdf?id=HJedXaEtvS)], [[project](https://github.com/xtinkt/editable)],  2020.7
   ![](https://img.shields.io/badge/image_classification-D8D0E1) ![](https://img.shields.io/badge/machine_translation-D8D0E1)

   Related work:
   - re-train on the original dataset augmented with new samples (expensive)
   - use a manual cache (e.g. lookup table) that overrules model predictions on problematic samples (can't generalize to semantic equivalent inputs)
   Method:
   Editable Training (employ meta-learning techniques to ensure that mistakes can be corrected without harming its overall performance)
   <!-- ![Alt text-w15](editable_train.png) -->

   <img src="img/editable_train.png" width="800" alt="webhooks">

2. **Fact-based Text Editing** ACL 2020. ![](https://img.shields.io/badge/Fact_based_text_editing-DCE7F1)

   *Hayate Iso, Chao Qiao, Hang Li*.  [[pdf](https://aclanthology.org/2020.acl-main.17.pdf)], [[project](https://github.com/isomap/factedit)],  2020.7
   ![](https://img.shields.io/badge/edit_draft_text_editing-D8D0E1)

   Main cons.:
   - propose a new dataset (task): transforms a draft text into a revised text based on given triples
   - new model, FACTEDITOR. The model consists of three components, a buffer for storing the draft text and its representations, a stream for storing the revised text and its representations, and a memory for storing the triples and their representations
   <!-- ![Alt text-w15](editable_train.png) -->

   <img src="img/fte.png" width="300" alt="webhooks">

2. Modifying Memories in Transformer Models
   Explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts.
    - Motivation
  - updating stale knowledge
  - protecting privacy
  - eliminating unintended biases (debias)
   - Baseline
  - Retraining the model on modified training set
  - Fine-tuning on modified facts
  - Fine-tuning on a mixture of modified and unmodified batches
 - Method
     - Constrained fine-tuning on supporting evidences for modified facts

3. Editing Factual Knowledge in Language Models
    - Evaluation
  - success rate: updates the knowledge
  - retain accuracy: retains the original predictions
  - equivalence accuracy: for semantically equivalent inputs,
  - performance deterioration: test performance deteriorates

4. Fast Model Editing at Scale
    a model editor efficient: if the time and memory requirements for computing Ï† and evaluating E are small.
gradients are highdimensional objects ðŸ‘‰ using a low-rank decomposition of the gradient

5. Locating and Editing Factual Knowledge in GPT (preprint)

### Continual Pretraining

1. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks (ACL 2020)
2. Mind the Gap: Assessing Temporal Generalization in Neural Language Models (NeurIps 2021)
3. Towards Continual Knowledge Learning of Language Models (preprint)
4. DEMix Layers: Disentangling Domains for Modular Language Modeling (preprint)
5. Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora (preprint)
6. Time-Aware Language Models as Temporal Knowledge Bases (preprint)
7. Dynamic Language Models for Continuously Evolving Content (KDD 2021)
8. ELLE: Efficient Lifelong Pre-training for Emerging Data (Findings of ACL 2022)

### Model Editing v.s. Continual Learning

- Common: assimilating or updating a modelâ€™s behavior without catastrophic forgetting

- Continual learning: learn a new task while preserving the performance on the previous tasks
wholly new behaviors or datasets
long sequences of model updates

- Model editing: explicitly modifying specific factual knowledge in models while ensuring the model performance does not degrade on the unmodified facts
considers an edit or batch of edits applied all at once
requires the model to memorize new facts that conflict with previously learned facts
